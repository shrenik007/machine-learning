{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  below is the dictionary which serves as values for 'params'\n",
    "param_grid_dict = {\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l2'],\n",
    "        'C': np.logspace(-4, 4, 20),\n",
    "        'solver': ['newton-cg', 'sag', 'saga']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': list(range(5, 21, 5)),\n",
    "        'max_features': list(range(5, 20, 15)),\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': list(range(1, 20, 2)),\n",
    "        'kernel': ['linear', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': list(range(500, 750, 1000)),\n",
    "        'max_features': list(range(5, 50, 5)),\n",
    "        'max_depth': [1, 10, 20, 30, 40]\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': [2, 5, 7, 10, 15, 20, 30, 40, 50],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2, 3, 5],\n",
    "    },\n",
    "    'BernoulliNB': {\n",
    "        'alpha': [1.0, 0.5, 0],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "    'GaussianNB': {},\n",
    "    'SGDClassifier': {\n",
    "        'loss': ['hinge', 'log', 'modified_huber'],\n",
    "        'penalty': [\"l2\", \"elasticnet\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_hot_encoding_column(data, column_name):\n",
    "    \"\"\"\n",
    "    While one-hot encoding solves the problem of unequal weights given to categories within a feature,\n",
    "    it is not very useful when there are many categories, as that will result in formation of as many new columns,\n",
    "    which can result in the curse of dimensionality.\n",
    "    The concept of the “curse of dimensionality” discusses that in\n",
    "    high-dimensional spaces some things just stop working properly.\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    X = enc.fit_transform(data[column_name].values.reshape(-1, 1)).toarray()\n",
    "    dfOneHot = pd.DataFrame(X, columns=[\"enc_\" + column_name.lower() + \"_\" + str(int(i)) for i in range(X.shape[1])])\n",
    "    data = pd.concat([data, dfOneHot], axis=1)\n",
    "    data = data.drop(column_name, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_handling_missing_values(data):\n",
    "    data = data.replace('?', np.NaN)\n",
    "    null_columns = data.columns[data.isnull().any()]\n",
    "    for column_name in null_columns:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        imp.fit(data[[column_name]])\n",
    "        data[column_name] = imp.transform(data[[column_name]]).ravel()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_handling_categorical_data(feature_data, label_data, encoder):\n",
    "    # 1. performing one hot encoding\n",
    "    feature_data = perform_one_hot_encoding_column(feature_data, 'workclass')\n",
    "    feature_data = perform_one_hot_encoding_column(feature_data, 'education')\n",
    "    feature_data = perform_one_hot_encoding_column(feature_data, 'marital-status')\n",
    "    feature_data = perform_one_hot_encoding_column(feature_data, 'occupation')\n",
    "    feature_data = perform_one_hot_encoding_column(feature_data, 'relationship')\n",
    "    feature_data = perform_one_hot_encoding_column(feature_data, 'race')\n",
    "    feature_data = perform_one_hot_encoding_column(feature_data, 'age_group')\n",
    "    # feature_data = perform_one_hot_encoding_column(feature_data, 'native-country')\n",
    "\n",
    "    # 2. performing label encoding\n",
    "    feature_data['enc_gender'] = feature_data['gender'].map({'Male': 1, 'Female': 0})\n",
    "    label_data = label_data.map({'<=50K': 1, '>50K': 0})\n",
    "    feature_data = feature_data.drop([\"gender\"], axis=1)\n",
    "    if encoder == 'OneHotEncoding':\n",
    "        #  part of RESEARCH\n",
    "        feature_data = perform_one_hot_encoding_column(feature_data, 'native-country')\n",
    "        feature_data = perform_one_hot_encoding_column(feature_data, 'capital-gain')\n",
    "        feature_data = perform_one_hot_encoding_column(feature_data, 'capital-loss')\n",
    "    elif encoder == 'LabelEncoder':\n",
    "        #  part of RESEARCH\n",
    "        label_enc = LabelEncoder()\n",
    "        feature_data['native-country'] = label_enc.fit_transform(feature_data['native-country'])\n",
    "        feature_data['capital-gain'] = label_enc.fit_transform(feature_data['capital-gain'])\n",
    "        feature_data['capital-loss'] = label_enc.fit_transform(feature_data['capital-loss'])\n",
    "    elif encoder == 'BinaryEncoder':\n",
    "        #  part of RESEARCH\n",
    "        binary_enc = ce.BinaryEncoder(cols=['native-country'])\n",
    "        native_country_enc = binary_enc.fit_transform(feature_data['native-country'])\n",
    "        feature_data = feature_data.drop('native-country', axis=1)\n",
    "\n",
    "        binary_enc = ce.BinaryEncoder(cols=['capital-gain'])\n",
    "        capital_gain_enc = binary_enc.fit_transform(feature_data['capital-gain'])\n",
    "        feature_data = feature_data.drop('capital-gain', axis=1)\n",
    "\n",
    "        binary_enc = ce.BinaryEncoder(cols=['capital-loss'])\n",
    "        capital_loss_enc = binary_enc.fit_transform(feature_data['capital-loss'])\n",
    "        feature_data = feature_data.drop('capital-loss', axis=1)\n",
    "\n",
    "        feature_data = pd.concat([feature_data, native_country_enc, capital_gain_enc, capital_loss_enc], axis=1)\n",
    "    else:\n",
    "        # 3. perform binary encoding on capital-gain and capital-loss column\n",
    "        # this method is implemented because the data is highly skewed and there are many unique values in the columns.\n",
    "        # if one-hot-encoding method is applied here then there will be dimentionality explosion.\n",
    "        # So binary encoding is applied where all the non-zero values are converted to 1 and 0's remain 0\n",
    "        feature_data['capital-gain'] = feature_data['capital-gain'].apply(lambda x: 1 if x != 0 else 0)\n",
    "        feature_data['capital-loss'] = feature_data['capital-loss'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "        # As there are many entries where there is United States, that's why data is highly skewed.\n",
    "        # I convert this to binary column that wherever there is United states it will be replaced by 1, otherwise 0\n",
    "        feature_data['native-country'] = feature_data['native-country'].apply(lambda x: 1 if x == 'United States' else 0)\n",
    "\n",
    "    print('Handling categorical data done')\n",
    "    return feature_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_age_groups(x):\n",
    "    binning_dict = {\n",
    "        tuple(range(10, 31)): '10-30',\n",
    "        tuple(range(31, 51)): '31-50',\n",
    "        tuple(range(51, 71)): '51-70',\n",
    "        tuple(range(71, 91)): '71-90',\n",
    "    }\n",
    "    # print(binning_dict.keys())\n",
    "    keys = binning_dict.keys()\n",
    "    for k in keys:\n",
    "        if x in k:\n",
    "            x = binning_dict[k]\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_outlier_detection(feature_data, column_name=\"all-columns\"):\n",
    "    sns.boxplot(data=feature_data.loc[:, ~feature_data.columns.str.contains('enc_') & ~feature_data.columns.str.contains('capital-') & ~feature_data.columns.str.contains('native')])\n",
    "    if column_name == \"all-columns\":\n",
    "        plt.xlabel(\"all df\")\n",
    "        plt.ylabel(\"All values\")\n",
    "    else:\n",
    "        plt.xlabel(\"df['\" + feature_data.name + \"']\")\n",
    "        plt.ylabel(str(feature_data.name))\n",
    "    plt.savefig(f'{column_name}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_scaling(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    data[['hours-per-week']] = scaler.fit_transform(data[['hours-per-week']])\n",
    "    columns = data.columns\n",
    "    if 'capital-gain' in columns:\n",
    "        data[['capital-gain']] = scaler.fit_transform(data[['capital-gain']])\n",
    "    if 'capital-loss' in columns:\n",
    "        data[['capital-loss']] = scaler.fit_transform(data[['capital-loss']])\n",
    "    data[['educational-num']] = scaler.fit_transform(data[['educational-num']])\n",
    "    print('data scaling done')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_standardization(feature_data):\n",
    "    scaler = StandardScaler()\n",
    "    scaler_data = scaler.fit_transform(feature_data)\n",
    "    new_data = pd.DataFrame(data=scaler_data, columns=feature_data.columns)\n",
    "    print('data standardization done')\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, encoder=\"auto\"):\n",
    "    \"\"\"\n",
    "    This function takes care of pre-processing of data. Various steps of pre-processing are handled here.\n",
    "    Drop some rows to reduce data size, converting age to age-groups, handling missing values,\n",
    "    handing categorical values; converting categorical values to numerical using encoding techniques, scaling of data,\n",
    "    data standardization and finally outlier detection.\n",
    "    Feature selection not be part of this process as majority of the columns are categorical columns.\n",
    "    \"\"\"\n",
    "    # reducing data. Consider only rows having Occupation as Armed-Forces, handlers-cleaners, Machine-op-inspct,\n",
    "    # Prof-specialty, Tech-support\n",
    "    # Get names of indexes for which column Age has value 30\n",
    "    rows_to_drop = [\"Adm-clerical\", \"Craft-repair\", \"Exec-managerial\", \"Farming-fishing\", \"Other-service\",\n",
    "                    \"Priv-house-serv\", \"Protective-serv\", \"Sales\", \"Transport-moving\"]\n",
    "    index_names = data[data['occupation'].isin(rows_to_drop)].index\n",
    "    # Delete these row indexes from dataFrame\n",
    "    data = data.drop(index_names, inplace=False)\n",
    "    data = data.reset_index()\n",
    "    data = data.drop('index', axis=1)\n",
    "\n",
    "    feature_data = data.drop(['income'], axis=1)\n",
    "    label_data = data['income']\n",
    "\n",
    "    # binning age column\n",
    "    feature_data['age_group'] = feature_data['age'].apply(lambda x: convert_to_age_groups(x))\n",
    "    # dropping unwanted_columns\n",
    "    feature_data = feature_data.drop(['fnlwgt', 'age'], axis=1)\n",
    "    # handling missing values\n",
    "    feature_data = perform_handling_missing_values(feature_data)\n",
    "    # converting categorical data to labels\n",
    "    feature_data, label_data = perform_handling_categorical_data(feature_data, label_data, encoder)\n",
    "    # scaling data\n",
    "    feature_data = perform_data_scaling(feature_data)\n",
    "    # standardizing data\n",
    "    feature_data = perform_data_standardization(feature_data)\n",
    "    if encoder == 'auto':\n",
    "        # outlier detection\n",
    "        perform_outlier_detection(feature_data)\n",
    "\n",
    "    # feature selection\n",
    "    # clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0, verbose=1)\n",
    "    # clf.fit(feature_data, label_data)\n",
    "    #\n",
    "    # feature_importance_list = clf.feature_importances_\n",
    "    #\n",
    "    # sorting_indices = np.argsort(feature_importance_list)\n",
    "    # print(sorting_indices)\n",
    "    print('data preprocessing completed')\n",
    "    return feature_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(train_features, train_labels, test_features, test_labels, clf):\n",
    "    \"\"\"\n",
    "    This is a generalised function which predicts values for the model object passed\n",
    "    \"\"\"\n",
    "    clf.fit(train_features, train_labels)\n",
    "    predicted = clf.score(test_features, test_labels)\n",
    "    return clf, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_performing_model(train_features, train_labels, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    In this function I am evaluating my data on 7 different classifiers. These are:\n",
    "    1. k-NN\n",
    "    2. Bernoulli Naive Bayes\n",
    "    3. Gaussian Naive Bayes\n",
    "    4. RandomForestClassifier\n",
    "    5. DecisionTreeClassifier\n",
    "    6. LogisticRegression\n",
    "    7. SGD\n",
    "    :param train_features:\n",
    "    :param train_labels:\n",
    "    :param test_features:\n",
    "    :param test_labels:\n",
    "    :return: classifier object\n",
    "    \"\"\"\n",
    "    classifier_dict = {}\n",
    "    print('------------------------Calculating KNN-------------------')\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    knn_clf, knn_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels, knn_clf)\n",
    "    classifier_dict[knn_clf] = knn_predicted\n",
    "    print('kNN accuracy=', knn_predicted)\n",
    "    #\n",
    "    print('-------------Calculating Bernoulli, Gaussian Naive Bayes-----------------')\n",
    "    nb_clf = BernoulliNB()\n",
    "    nb_clf, bernoulli_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels, nb_clf)\n",
    "    classifier_dict[nb_clf] = bernoulli_predicted\n",
    "    print('BernoulliNB accuracy=', bernoulli_predicted)\n",
    "\n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf, gaussian_nb_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels, nb_clf)\n",
    "    classifier_dict[nb_clf] = gaussian_nb_predicted\n",
    "    print('GaussianNB accuracy=', gaussian_nb_predicted)\n",
    "\n",
    "    print('------------------Calculating RandomForestClassifier-----------------')\n",
    "    rf_clf = RandomForestClassifier(random_state=0)\n",
    "    rf_clf, random_forest_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels,\n",
    "                                                         rf_clf)\n",
    "    classifier_dict[rf_clf] = random_forest_predicted\n",
    "    print('RandomForestClassifier accuracy=', random_forest_predicted)\n",
    "\n",
    "    print('------------------Calculating DecisionTreeClassifier-----------------')\n",
    "    dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "    dt_clf, decision_tree_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels,\n",
    "                                                         dt_clf)\n",
    "    classifier_dict[dt_clf] = decision_tree_predicted\n",
    "    print('DecisionTreeClassifier accuracy=', decision_tree_predicted)\n",
    "\n",
    "    print('------------------Calculating LogisticRegression-----------------')\n",
    "    lr_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "    lr_clf, logistic_regression_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels,\n",
    "                                                               lr_clf)\n",
    "    classifier_dict[lr_clf] = logistic_regression_predicted\n",
    "    print('LogisticRegression accuracy=', logistic_regression_predicted)\n",
    "\n",
    "    # print('------------------Calculating SVC-----------------')\n",
    "    # svc_clf = SVC(random_state=0, C=1.0)\n",
    "    # svc_clf, svc_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels, svc_clf)\n",
    "    # classifier_dict[svc_clf] = svc_predicted\n",
    "    # print('SVC accuracy=', svc_predicted)\n",
    "\n",
    "    print('------------------Calculating SGD-----------------')\n",
    "    sgd_clf = SGDClassifier(random_state=0)\n",
    "    sgd_clf, sgd_predicted = get_model_accuracy(train_features, train_labels, test_features, test_labels,\n",
    "                                                sgd_clf)\n",
    "    classifier_dict[sgd_clf] = sgd_predicted\n",
    "    print('Stochastic Gradient Descent accuracy=', sgd_predicted)\n",
    "\n",
    "    max_accuracy_classifier = dict(sorted(classifier_dict.items(), key=operator.itemgetter(1), reverse=True)[:3])\n",
    "    return max_accuracy_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling categorical data done\n",
      "data scaling done\n",
      "data standardization done\n",
      "data preprocessing completed\n",
      "Now splitting data...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the main driver function which calls different functions and gets output which is fed as input to other.\n",
    "This function performs data preprocessing, splits train and test data, calculates accuracy of 8 different ML\n",
    "models, selects top 3 models, performs hyper-parameter tuning on these models, calculates confusion matrix,\n",
    "displays classification report, performs research topic which is implementing different encoding techniques,\n",
    "displays the accuracy for each encoding technique used.\n",
    "\"\"\"\n",
    "main_data = pd.read_csv(\"adult.csv\")  # reading csv file\n",
    "feature_data, label_data = preprocess_data(main_data)  # pre-processing data to make it stable\n",
    "\n",
    "print('Now splitting data...')\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(feature_data, label_data, test_size=0.2,\n",
    "                                                                            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculates accuracy for 8 different models and return top 3 performing models\n",
    "top_models = get_top_performing_model(train_features, train_labels, test_features, test_labels)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to accuracies top performing models are:\n",
      "----------------------- LogisticRegression ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.8507337796086509\n",
      "Best Params:  {'C': 0.23357214690901212, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Confusion matrix for the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2587 1397 926 10626\n",
      "Classification report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.8838595052066663\n",
      "Average recall score: 0.9198404642185478\n",
      "Average f1 score: 0.901473369471467\n",
      "----------------------- SGDClassifier ----------------------\n",
      "Best Score:  0.8448764160659115\n",
      "Best Params:  {'loss': 'log', 'penalty': 'elasticnet'}\n",
      "Confusion matrix for the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708 1276 1139 10413\n",
      "Classification report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.8908526117835306\n",
      "Average recall score: 0.9014023231161638\n",
      "Average f1 score: 0.8960921488865377\n",
      "----------------------- KNeighborsClassifier ----------------------\n",
      "Best Score:  0.8432028836251287\n",
      "Best Params:  {'algorithm': 'brute', 'n_neighbors': 50, 'p': 1}\n",
      "Confusion matrix for the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600 1384 1028 10524\n",
      "Classification report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/shrenik/ai_course/ML/SD_R00183334_Assignment2/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(f'According to accuracies top performing models are:')\n",
    "#  the below part of code will perform hyper-parameter tuning for the top 3 models selected and calculate accuracy\n",
    "for model in top_models:\n",
    "    # if type(model).__name__ == 'LogisticRegression':\n",
    "    print('-----------------------', type(model).__name__, '----------------------')\n",
    "    gridS = GridSearchCV(model, param_grid=param_grid_dict[type(model).__name__], cv=5, verbose=False, n_jobs=-1)\n",
    "    grid_result = gridS.fit(feature_data, label_data)\n",
    "\n",
    "    print('Best Score: ', grid_result.best_score_)\n",
    "    print('Best Params: ', grid_result.best_params_)\n",
    "    print('Confusion matrix for the model')\n",
    "    y_pred = cross_val_predict(gridS, feature_data, label_data)\n",
    "    true_neg, false_pos, false_neg, true_pos = confusion_matrix(label_data, y_pred).ravel()\n",
    "    print(true_neg, false_pos, false_neg, true_pos)\n",
    "    print('Classification report')\n",
    "    precision_scores = cross_val_score(gridS, feature_data, label_data, scoring='precision')\n",
    "    recall_scores = cross_val_score(gridS, feature_data, label_data, scoring='recall')\n",
    "    f_scores = cross_val_score(gridS, feature_data, label_data, scoring='f1')\n",
    "    print('Average precision score:', precision_scores.mean())\n",
    "    print('Average recall score:', recall_scores.mean())\n",
    "    print('Average f1 score:', f_scores.mean())\n",
    "\n",
    "print('Evaluated baseline models')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the below part of code will now take top 3 models and change the encoding techniques and notes down accuracy for\n",
    "#  each model when used with a particular encoder\n",
    "print('------------Researching different encoding techniques----------------')\n",
    "encoder_list = ['LabelEncoder', 'OneHotEncoding', 'BinaryEncoder']\n",
    "for model in top_models:\n",
    "    # if type(model).__name__ == 'LogisticRegression':\n",
    "    for encoder in encoder_list:\n",
    "        print('-----------------------', type(model).__name__, '----------------------')\n",
    "        print(f\"Encoder={encoder}\")\n",
    "        feature_data, label_data = preprocess_data(main_data, encoder=encoder)\n",
    "        gridS = GridSearchCV(model, param_grid=param_grid_dict[type(model).__name__], cv=5, verbose=False, n_jobs=-1)\n",
    "        grid_result = gridS.fit(feature_data, label_data)\n",
    "        print('Best Score: ', grid_result.best_score_)\n",
    "        print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
